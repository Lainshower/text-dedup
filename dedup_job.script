#!/bin/bash

#
# parameters
#
#SBATCH --job-name=dedup_multidir_joonwon.jang
#SBATCH --output=/lab-exaone/joonwon.jang/lg_stack/text-dedup/log_dedup_%j_%u_%x.out
#SBATCH --error=/lab-exaone/joonwon.jang/lg_stack/text-dedup/log_dedup_%j_%u_%x.err
#SBATCH --chdir=/lab-exaone/joonwon.jang/
#SBATCH --partition=all
#SBATCH --nodes=12
#SBATCH --nodelist=cluster-data-[03-14]

# Input/output directories (space-separated for multiple filtering outputs)
input_dirs='/cl_data/exaone/joonwon.jang/pretrain/output_dir1 /cl_data/exaone/joonwon.jang/pretrain/output_dir2'
output_dir='/cl_data/exaone/joonwon.jang/pretrain/final_dedup_output'

# Resource and deduplication parameters
n_nodes=12
n_cores=128
batch_size=10000
threshold=0.8
num_perm=256
ngram=3
min_length=5
hash_bits=64

# Activate conda environment and run deduplication
srun bash -c "source opc/bin/activate; cd lg_stack/text-dedup; python -m text_dedup.minhash_mn \
    --input_dirs \"${input_dirs}\" \
    --output \"${output_dir}\" \
    --batch_size ${batch_size} \
    --num_proc ${n_cores} \
    --threshold ${threshold} \
    --num_perm ${num_perm} \
    --ngram ${ngram} \
    --min_length ${min_length} \
    --hash_bits ${hash_bits}\""